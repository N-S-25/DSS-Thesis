{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVN4O-_h_Lh5"
      },
      "source": [
        "**1. Linear Models: Logestic regression**\n",
        "\n",
        "**2. Non-linear models: Random forest, LightGBM, SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjjchXb__DvE"
      },
      "outputs": [],
      "source": [
        "### Packages ######\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "id": "RIuffkl8VDRM",
        "outputId": "b9a0bc18-181b-4ae7-ef7e-b856bc3079a9"
      },
      "outputs": [],
      "source": [
        "#checking gender imbalance across drug conditions\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "\n",
        "# Preview the column names to identify relevant ones\n",
        "print(df.columns)\n",
        "\n",
        "# Assuming the columns are named 'Gender' and 'DrugCondition'\n",
        "# If they differ, replace with actual column names from the printout\n",
        "\n",
        "# Count plot to visualize distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=df, x='cond', hue='gender')\n",
        "plt.title('Distribution of Drug Conditions Across Gender')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Crosstab for proportions\n",
        "crosstab = pd.crosstab(df['cond'], df['gender'], normalize='index')\n",
        "print(\"Proportion of Gender within each Drug Condition:\")\n",
        "print(crosstab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPlxH2tmDJrW"
      },
      "source": [
        "**1.1. Logestic Regression (5-folds)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-J_tioucB3G_",
        "outputId": "7bce4f0a-73eb-489d-efcb-a5df942990ad"
      },
      "outputs": [],
      "source": [
        "# ===  Load dataset ===\n",
        "df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# ===  Map education into 'low' and 'high' ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "df[\"education\"] = df[\"education\"].replace(education_map)\n",
        "\n",
        "# ===  Drop unnecessary columns ===\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\"\n",
        "]\n",
        "X = df.drop(columns=drop_cols + [\"cond\"])\n",
        "y = df[\"cond\"]\n",
        "\n",
        "# ===  Encode target ===\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# ===  Define feature groups ===\n",
        "binary_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "scaled_cols = [\"income\", \"age\"]\n",
        "categorical_cols = [\"gender\", \"education\", \"kproto_cluster\",\n",
        "                    \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "\n",
        "# ===  Preprocess features ===\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"scale\", StandardScaler(), scaled_cols),\n",
        "    (\"onehot\", OneHotEncoder(drop=\"first\", sparse_output=False), categorical_cols)\n",
        "], remainder='passthrough')\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "cat_feature_names = preprocessor.named_transformers_[\"onehot\"].get_feature_names_out(categorical_cols)\n",
        "all_feature_names = list(cat_feature_names) + scaled_cols + binary_cols\n",
        "X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names)\n",
        "\n",
        "# ===  Fit base logistic regression to get top 20 features ===\n",
        "X_train_base, _, y_train_base, _ = train_test_split(\n",
        "    X_processed_df, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)\n",
        "\n",
        "log_reg_base = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
        "log_reg_base.fit(X_train_base, y_train_base)\n",
        "coef_importance = pd.Series(log_reg_base.coef_[0], index=all_feature_names)\n",
        "\n",
        "# ===  Select top 20 features ===\n",
        "top_20 = coef_importance.abs().sort_values(ascending=False).head(20).index.tolist()\n",
        "\n",
        "# ===  Create valid interaction terms (only if both columns exist) ===\n",
        "interaction_candidates = [\n",
        "    (\"kproto_cluster_1\", \"education_high\"),\n",
        "    (\"kproto_cluster_2\", \"education_high\"),\n",
        "    (\"pred_cluster_block1_1\", \"gender_2\"),\n",
        "    (\"pred_cluster_block1_2\", \"gender_2\")\n",
        "]\n",
        "\n",
        "valid_interactions = []\n",
        "for col1, col2 in interaction_candidates:\n",
        "    if col1 in X_processed_df.columns and col2 in X_processed_df.columns:\n",
        "        new_col = f\"{col1}__x__{col2}\"\n",
        "        X_processed_df[new_col] = X_processed_df[col1] * X_processed_df[col2]\n",
        "        valid_interactions.append(new_col)\n",
        "\n",
        "# ===  Combine top features and interactions ===\n",
        "selected_features = top_20 + valid_interactions\n",
        "X_selected = X_processed_df[selected_features]\n",
        "\n",
        "# ===  Train/test split (stratified by gender) ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y_encoded, test_size=0.2, stratify=X[\"gender\"], random_state=42)\n",
        "\n",
        "# ===  Fit and evaluate logistic regression ===\n",
        "log_reg = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression with Interactions\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_acc = cross_val_score(log_reg, X_train, y_train, cv=skf, scoring=\"accuracy\")\n",
        "cv_f1 = cross_val_score(log_reg, X_train, y_train, cv=skf, scoring=\"f1_macro\")\n",
        "\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Test Macro F1: {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
        "print(f\"CV Accuracy (mean ± std): {cv_acc.mean():.3f} ± {cv_acc.std():.3f}\")\n",
        "print(f\"CV Macro F1 (mean ± std): {cv_f1.mean():.3f} ± {cv_f1.std():.3f}\")\n",
        "\n",
        "################ visualizing the feature space to see how features are distirbuted\n",
        "################ and why the model is unable to make any predictions on the class C############\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_selected)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y_encoded, cmap='tab10', alpha=0.7)\n",
        "plt.title(\"PCA of Selected Features\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.colorbar(label=\"Class\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhxAXp4RdAHj",
        "outputId": "0b7bcb94-5386-4f03-bd26-4ec78d8d9c5e"
      },
      "outputs": [],
      "source": [
        "# === Extract absolute coefficients for Logistic Regression feature importance ===\n",
        "importance_series = pd.Series(\n",
        "    log_reg.coef_[0], index=X_selected.columns\n",
        ").abs().sort_values(ascending=False)\n",
        "\n",
        "# === Display top 20 features ===\n",
        "top_n = 20\n",
        "top_features_lr = importance_series.head(top_n)\n",
        "\n",
        "print(f\"\\nTop {top_n} Logistic Regression Most Important Features:\\n\")\n",
        "print(top_features_lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "L6T7abhSBZyk",
        "outputId": "e1c3542f-e612-4613-af1f-f7392a15bd11"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# === Get feature importance from trained model ===\n",
        "feature_importance = pd.Series(\n",
        "    np.abs(log_reg.coef_[0]),  # absolute value of coefficients\n",
        "    index=X_selected.columns\n",
        ").sort_values(ascending=True)\n",
        "\n",
        "# === Plot ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "feature_importance.plot(kind='barh', color='steelblue')\n",
        "plt.title(\"Feature Importance - Logistic Regression\")\n",
        "plt.xlabel(\"Absolute Coefficient Value\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAZgMKQ1EIdg"
      },
      "source": [
        "**2.1. Random Forest (3 and 5 folds)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L2dCthwuEN8O",
        "outputId": "04e0054a-31c7-4940-ad5c-f0d8805d6250"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "\n",
        "# ===  Map education and create interaction features ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "df[\"education\"] = df[\"education\"].replace(education_map)\n",
        "\n",
        "# Create new interaction features\n",
        "df['age_income_interaction'] = df['age'] * df['income']\n",
        "\n",
        "# Convert boolean columns to integers\n",
        "bool_cols_to_convert = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "for col in bool_cols_to_convert:\n",
        "    df[col] = df[col].astype(int)\n",
        "\n",
        "# ===  Drop unnecessary columns ===\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\"\n",
        "]\n",
        "X = df.drop(columns=drop_cols + [\"cond\"])\n",
        "y = df[\"cond\"]\n",
        "\n",
        "# ===  Encode target ===\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# ===  Define feature groups (updated with new interaction feature) ===\n",
        "scaled_cols = [\"income\", \"age\", \"age_income_interaction\"]\n",
        "binary_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "categorical_cols = [\"gender\", \"education\", \"kproto_cluster\",\n",
        "                    \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "\n",
        "# Create a column transformer to apply different transformations\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), scaled_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Create a pipeline with the preprocessor and the Random Forest model\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# ===  Hyperparameter tuning using GridSearchCV ===\n",
        "print(\"Performing hyperparameter tuning with GridSearchCV...\")\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [5, 10, 20, None],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "cv_grid = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(model_pipeline, param_grid, cv=cv_grid, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "\n",
        "grid_search.fit(X, y_encoded)\n",
        "\n",
        "print(\"\\nBest hyperparameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "\n",
        "# Get the best model from the grid search\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# === Define evaluation function ===\n",
        "def evaluate_model(model, X, y, folds):\n",
        "    print(f\"\\n=== {folds}-Fold Cross-Validation ===\")\n",
        "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "\n",
        "    acc_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "    f1_scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')\n",
        "\n",
        "    for i, (acc, f1) in enumerate(zip(acc_scores, f1_scores), 1):\n",
        "        print(f\"Fold {i}: Accuracy = {acc:.4f}, Macro F1 = {f1:.4f}\")\n",
        "\n",
        "    # print(f\"\\nAverage Accuracy: {acc_scores.mean():.4f}\")\n",
        "    # print(f\"Average Macro F1: {f1_scores.mean():.4f}\")\n",
        "\n",
        "      # Print results\n",
        "        print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "        print(f\"Test Macro F1: {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
        "        print(f\"CV Accuracy (mean ± std): {cv_acc.mean():.3f} ± {cv_acc.std():.3f}\")\n",
        "        print(f\"CV Macro F1 (mean ± std): {cv_f1.mean():.3f} ± {cv_f1.std():.3f}\")\n",
        "\n",
        "# === Run evaluations ===\n",
        "evaluate_model(best_model, X, y_encoded, folds=3)\n",
        "evaluate_model(best_model, X, y_encoded, folds=5)\n",
        "# ===  Plot feature importance with the best model and new features ===\n",
        "print(\"\\nPlotting feature importance with the best model...\")\n",
        "\n",
        "# Get feature names after preprocessing\n",
        "ohe_feature_names = best_model.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
        "passthrough_feature_names = list(binary_cols)\n",
        "preprocessed_feature_names = scaled_cols + list(ohe_feature_names) + passthrough_feature_names\n",
        "\n",
        "# Get feature importances from the best Random Forest classifier\n",
        "feature_importances = best_model.named_steps['classifier'].feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': preprocessed_feature_names,\n",
        "    'Importance': feature_importances\n",
        "})\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 15 features\n",
        "print(\"\\nTop 15 Feature Importances with tuned model:\")\n",
        "print(importance_df.head(15))\n",
        "\n",
        "# Plotting the feature importances\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importance - Random Forest\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"tuned_feature_importance.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "s4gZbR70UnFU",
        "outputId": "ebe08a86-a96b-4869-f626-eb07e35dfc0e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# === Train/Test Split ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# === Fit best model on training set ===\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# === Predict on test set ===\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# === Classification Report ===\n",
        "print(\"\\n=== Classification Report (Random Forest) ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# === Confusion Matrix ===\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Random Forest (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov32qcW3HY-S"
      },
      "source": [
        "**2.2. LightGBM (3 and 5 folds)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VJ7T5LLBHd5h",
        "outputId": "1dcde738-d94a-4dcf-c808-c474762bdaee"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === 1. Load dataset ===\n",
        "df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# === 2. Map education into 'low' and 'high' ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "df[\"education\"] = df[\"education\"].replace(education_map)\n",
        "\n",
        "# === 3. Drop unnecessary columns ===\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\"\n",
        "]\n",
        "X = df.drop(columns=drop_cols + [\"cond\"])\n",
        "y = df[\"cond\"]\n",
        "\n",
        "# === 4. Encode target ===\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# === 5. Define feature groups ===\n",
        "scaled_cols = [\"income\", \"age\"]\n",
        "binary_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "categorical_cols = [\"gender\", \"education\", \"kproto_cluster\",\n",
        "                    \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "\n",
        "# === 6. Preprocess features ===\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"scale\", StandardScaler(), scaled_cols),\n",
        "    (\"onehot\", OneHotEncoder(drop=\"first\", sparse_output=False), categorical_cols)\n",
        "], remainder='passthrough')\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "cat_feature_names = preprocessor.named_transformers_[\"onehot\"].get_feature_names_out(categorical_cols)\n",
        "all_feature_names = list(cat_feature_names) + scaled_cols + binary_cols\n",
        "X_processed_df = pd.DataFrame(X_processed, columns=all_feature_names)\n",
        "\n",
        "# === 7. Add interaction terms ===\n",
        "interaction_candidates = [\n",
        "    (\"kproto_cluster_1\", \"education_high\"),\n",
        "    (\"kproto_cluster_2\", \"education_high\"),\n",
        "    (\"pred_cluster_block1_1\", \"gender_2\"),\n",
        "    (\"pred_cluster_block1_2\", \"gender_2\"),\n",
        "    (\"income\", \"education_high\"),\n",
        "    (\"age\", \"education_high\")\n",
        "]\n",
        "\n",
        "for col1, col2 in interaction_candidates:\n",
        "    if col1 in X_processed_df.columns and col2 in X_processed_df.columns:\n",
        "        new_col = f\"{col1}__x__{col2}\"\n",
        "        X_processed_df[new_col] = X_processed_df[col1] * X_processed_df[col2]\n",
        "\n",
        "# === 8. Train/Test Split ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_processed_df, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# === 9. Define and Tune LightGBM ===\n",
        "print(\"\\n=== Tuning LightGBM with GridSearchCV on Training Set ===\")\n",
        "model = LGBMClassifier(random_state=42, class_weight='balanced')\n",
        "\n",
        "lgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.7, 0.8, 0.9]\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=lgb_param_grid,\n",
        "    cv=cv,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "val_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\nBest LightGBM Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validated Macro F1 Score (Train CV): {val_score:.3f}\")\n",
        "\n",
        "# === 10. Define Evaluation Function ===\n",
        "def evaluate_model(model, X, y, folds):\n",
        "    print(f\"\\n=== {folds}-Fold Cross-Validation ===\")\n",
        "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "\n",
        "    acc_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "    f1_scores = cross_val_score(model, X, y, cv=skf, scoring='f1_macro')\n",
        "\n",
        "    for i, (acc, f1) in enumerate(zip(acc_scores, f1_scores), 1):\n",
        "        print(f\"Fold {i}: Accuracy = {acc:.4f}, Macro F1 = {f1:.4f}\")\n",
        "\n",
        "    avg_acc = acc_scores.mean()\n",
        "    avg_f1 = f1_scores.mean()\n",
        "\n",
        "    # print(f\"\\nAverage Accuracy: {avg_acc:.4f}\")\n",
        "    # print(f\"Average Macro F1: {avg_f1:.4f}\")\n",
        "\n",
        "    ## Print results\n",
        "    print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "    print(f\"Test Macro F1: {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
        "    print(f\"CV Accuracy (mean ± std): {cv_acc.mean():.3f} ± {cv_acc.std():.3f}\")\n",
        "    print(f\"CV Macro F1 (mean ± std): {cv_f1.mean():.3f} ± {cv_f1.std():.3f}\")\n",
        "\n",
        "    return avg_acc, avg_f1\n",
        "\n",
        "# === 11. Run Evaluations ===\n",
        "acc_3, f1_3 = evaluate_model(best_model, X_processed_df, y_encoded, folds=3)\n",
        "acc_5, f1_5 = evaluate_model(best_model, X_processed_df, y_encoded, folds=5)\n",
        "\n",
        "# === 12. Evaluate on Held-Out Test Set ===\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nTest Accuracy (Held-Out): {test_accuracy:.3f}\")\n",
        "print(\"\\n=== Classification Report (Test Set) ===\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "# === 13. Confusion Matrix ===\n",
        "cm_test = confusion_matrix(y_test, y_test_pred)\n",
        "disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=label_encoder.classes_)\n",
        "disp_test.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - LightGBM (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === 14. Feature Importance Plot ===\n",
        "print(\"\\n=== Feature Importances ===\")\n",
        "importances = best_model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=importances[indices][:20], y=np.array(X_processed_df.columns)[indices][:20], palette=\"Blues\")\n",
        "plt.title(\"Feature Importances - LightGBM\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === 15. Final Summary ===\n",
        "print(\"\\n=== Final Summary ===\")\n",
        "print(f\"Validation Macro F1 (Train CV - GridSearch): {val_score:.3f}\")\n",
        "print(f\"Test Accuracy (Held-Out): {test_accuracy:.3f}\")\n",
        "print(f\"\\nCross-Validation Results:\")\n",
        "print(f\"  3-Fold CV: Accuracy = {acc_3:.3f}, Macro F1 = {f1_3:.3f}\")\n",
        "print(f\"  5-Fold CV: Accuracy = {acc_5:.3f}, Macro F1 = {f1_5:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h032nX9gGci"
      },
      "outputs": [],
      "source": [
        "# === Print top 20 most important features ===\n",
        "light_top_20_features = pd.DataFrame({\n",
        "    \"Feature\": np.array(X_processed_df.columns)[indices][:20],\n",
        "    \"Importance\": importances[indices][:20]\n",
        "})\n",
        "print(\"\\nTop 20 Most Important Features:\\n\")\n",
        "print(light_top_20_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_uo_11UHwEK"
      },
      "source": [
        "**2.3. SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFtBy_DhH0cd",
        "outputId": "69bbfd06-b716-44de-a07f-78d1efd3887f"
      },
      "outputs": [],
      "source": [
        "########### SVM 3-folds #############\n",
        "# === 1. Load dataset ===\n",
        "try:\n",
        "    df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'modeling_dataset_with_demo_and_age.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    # Exiting the script gracefully if the file is not found\n",
        "    exit()\n",
        "\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# === 2. Split data before any processing to prevent data leakage ===\n",
        "X = df.drop(columns=[\"cond\"])\n",
        "y = df[\"cond\"]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# === 3. Map education and create interaction features ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "X_train[\"education\"] = X_train[\"education\"].replace(education_map)\n",
        "X_test[\"education\"] = X_test[\"education\"].replace(education_map)\n",
        "\n",
        "X_train['age_income_interaction'] = X_train['age'] * X_train['income']\n",
        "X_test['age_income_interaction'] = X_test['age'] * X_test['income']\n",
        "\n",
        "bool_cols_to_convert = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "for col in bool_cols_to_convert:\n",
        "    X_train[col] = X_train[col].astype(int)\n",
        "    X_test[col] = X_test[col].astype(int)\n",
        "\n",
        "# === 4. Drop unnecessary columns ===\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\"\n",
        "]\n",
        "X_train = X_train.drop(columns=drop_cols)\n",
        "X_test = X_test.drop(columns=drop_cols)\n",
        "\n",
        "# === 5. Define feature groups ===\n",
        "scaled_cols = [\"income\", \"age\", \"age_income_interaction\"]\n",
        "categorical_cols = [\"gender\", \"education\", \"kproto_cluster\",\n",
        "                    \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "\n",
        "# === 6. Preprocess features and define SVM within a Pipeline ===\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), scaled_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', drop=\"first\"), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(random_state=42))\n",
        "])\n",
        "\n",
        "# === 7. Define and Tune SVM with an Expanded Grid ===\n",
        "print(\"\\n=== Tuning SVM with an expanded GridSearchCV on Training Set ===\")\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'classifier__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'classifier__kernel': ['rbf']\n",
        "}\n",
        "\n",
        "cv_grid = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_grid,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV on the TRAINING data only\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "val_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\n Best SVM Parameters: {grid_search.best_params_}\")\n",
        "print(f\" Best Cross-Validated Macro F1 Score (Train CV): {val_score:.3f}\")\n",
        "\n",
        "# === 8. Evaluate on Held-Out Test Set ===\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\n Test Accuracy (Held-Out): {test_accuracy:.3f}\")\n",
        "print(\"\\n=== Classification Report (Test Set) ===\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9vl3rzMPw5B",
        "outputId": "dcebd76b-a9c6-4a9d-c753-ad431dca0fa8"
      },
      "outputs": [],
      "source": [
        "################ SVM 5-folds #################\n",
        "\n",
        "# === 1. Load dataset ===\n",
        "try:\n",
        "    df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'modeling_dataset_with_demo_and_age.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    # Exiting the script gracefully if the file is not found\n",
        "    exit()\n",
        "\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# === 2. Split data before any processing to prevent data leakage ===\n",
        "X = df.drop(columns=[\"cond\"])\n",
        "y = df[\"cond\"]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# === 3. Map education and create interaction features ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "X_train[\"education\"] = X_train[\"education\"].replace(education_map)\n",
        "X_test[\"education\"] = X_test[\"education\"].replace(education_map)\n",
        "\n",
        "X_train['age_income_interaction'] = X_train['age'] * X_train['income']\n",
        "X_test['age_income_interaction'] = X_test['age'] * X_test['income']\n",
        "\n",
        "bool_cols_to_convert = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "for col in bool_cols_to_convert:\n",
        "    X_train[col] = X_train[col].astype(int)\n",
        "    X_test[col] = X_test[col].astype(int)\n",
        "\n",
        "# === 4. Drop unnecessary columns ===\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\"\n",
        "]\n",
        "X_train = X_train.drop(columns=drop_cols)\n",
        "X_test = X_test.drop(columns=drop_cols)\n",
        "\n",
        "# === 5. Define feature groups ===\n",
        "scaled_cols = [\"income\", \"age\", \"age_income_interaction\"]\n",
        "categorical_cols = [\"gender\", \"education\", \"kproto_cluster\",\n",
        "                    \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "\n",
        "# === 6. Preprocess features and define SVM within a Pipeline ===\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), scaled_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', drop=\"first\"), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(random_state=42))\n",
        "])\n",
        "\n",
        "# === 7. Define and Tune SVM with an Expanded Grid ===\n",
        "print(\"\\n=== Tuning SVM with an expanded GridSearchCV on Training Set ===\")\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'classifier__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'classifier__kernel': ['rbf']\n",
        "}\n",
        "\n",
        "cv_grid = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_grid,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV on the TRAINING data only\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "val_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\n Best SVM Parameters: {grid_search.best_params_}\")\n",
        "print(f\"\\n Cross-Validated Macro F1 Score (Train CV): {np.mean(val_score):.3f} ± {np.std(val_score):.3f}\")\n",
        "\n",
        "# === 8. Evaluate on Held-Out Test Set ===\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\n Test Accuracy (Held-Out): {test_accuracy:.3f}\")\n",
        "print(\"\\n=== Classification Report (Test Set) ===\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAcKfCdDlHmt",
        "outputId": "bf0522e5-5dc6-4fa3-a4c5-c137d175ea73"
      },
      "outputs": [],
      "source": [
        "# === 1. Load and preprocess dataset ===\n",
        "try:\n",
        "    df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'modeling_dataset_with_demo_and_age.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "X = df.drop(columns=[\"cond\"])\n",
        "y = df[\"cond\"]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
        ")\n",
        "\n",
        "# === 2. Feature engineering ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "X_train[\"education\"] = X_train[\"education\"].replace(education_map)\n",
        "X_test[\"education\"] = X_test[\"education\"].replace(education_map)\n",
        "\n",
        "X_train['age_income_interaction'] = X_train['age'] * X_train['income']\n",
        "X_test['age_income_interaction'] = X_test['age'] * X_test['income']\n",
        "\n",
        "bool_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "for col in bool_cols:\n",
        "    X_train[col] = X_train[col].astype(int)\n",
        "    X_test[col] = X_test[col].astype(int)\n",
        "\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\"\n",
        "]\n",
        "X_train = X_train.drop(columns=drop_cols)\n",
        "X_test = X_test.drop(columns=drop_cols)\n",
        "\n",
        "scaled_cols = [\"income\", \"age\", \"age_income_interaction\"]\n",
        "categorical_cols = [\"gender\", \"education\", \"kproto_cluster\",\n",
        "                    \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "\n",
        "# === 3. Define pipeline and parameter grid ===\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), scaled_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', drop=\"first\"), categorical_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(random_state=42))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'classifier__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'classifier__kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# === 4. Run GridSearchCV with 3-fold and 5-fold ===\n",
        "for folds in [3, 5]:\n",
        "    print(f\"\\n=== Tuning SVM with {folds}-fold Cross-Validation ===\")\n",
        "    cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model_pipeline,\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring='f1_macro',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    val_score = grid_search.best_score_\n",
        "    print(f\"\\n Best Parameters ({folds}-fold): {grid_search.best_params_}\")\n",
        "    print(f\" Best Macro F1 Score (Train CV): {val_score:.3f}\")\n",
        "\n",
        "    # === Evaluate on Test Set ===\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f\"\\n Test Accuracy ({folds}-fold model): {test_accuracy:.3f}\")\n",
        "    print(\"\\n=== Classification Report (Test Set) ===\")\n",
        "    print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2w-z9rxht-j",
        "outputId": "44b635b0-7a11-4083-b788-250f83d6a187"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# === After each GridSearchCV ===\n",
        "for folds in [3, 5]:\n",
        "    print(f\"\\n=== Tuning SVM with {folds}-fold Cross-Validation ===\")\n",
        "    cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model_pipeline,\n",
        "        param_grid=param_grid,\n",
        "        cv=cv,\n",
        "        scoring='f1_macro',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    val_score = grid_search.best_score_\n",
        "    cv_results = pd.DataFrame(grid_search.cv_results_)\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    mask = (\n",
        "        (cv_results['param_classifier__C'] == best_params['classifier__C']) &\n",
        "        (cv_results['param_classifier__gamma'] == best_params['classifier__gamma']) &\n",
        "        (cv_results['param_classifier__kernel'] == best_params['classifier__kernel'])\n",
        "    )\n",
        "\n",
        "    cv_macro_f1_mean = cv_results.loc[mask, 'mean_test_score'].values[0]\n",
        "    cv_macro_f1_std = cv_results.loc[mask, 'std_test_score'].values[0]\n",
        "\n",
        "    acc_scores = cross_val_score(best_model, X_train, y_train, cv=cv, scoring='accuracy')\n",
        "    cv_acc_mean = acc_scores.mean()\n",
        "    cv_acc_std = acc_scores.std()\n",
        "\n",
        "    y_test_pred = best_model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "    print(f\"\\n=== SVM Performance Summary ({folds}-fold CV) ===\")\n",
        "    print(f\"CV Accuracy (Mean ± Std): {cv_acc_mean:.3f} ± {cv_acc_std:.3f}\")\n",
        "    print(f\"CV Macro F1 (Mean ± Std): {cv_macro_f1_mean:.3f} ± {cv_macro_f1_std:.3f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "    print(f\"Test Macro F1: {test_macro_f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "ff6o_NziGKJk",
        "outputId": "f6dde46a-1407-473d-91d5-d8a433ad6e71"
      },
      "outputs": [],
      "source": [
        "# === Generate Confusion Matrix ===\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# === Plot Confusion Matrix as Heatmap ===\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=label_encoder.classes_,\n",
        "            yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix - SVM Model')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ8lBSWRiN4Y",
        "outputId": "f93da345-5c6e-4680-fefc-11dc7db144ea"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# === Manually transform X_test using the preprocessor ===\n",
        "X_test_transformed = best_model.named_steps['preprocessor'].transform(X_test)\n",
        "\n",
        "# === Apply permutation importance to the classifier only ===\n",
        "perm_result = permutation_importance(\n",
        "    best_model.named_steps['classifier'], X_test_transformed, y_test,\n",
        "    n_repeats=10, scoring='f1_macro', random_state=42, n_jobs=-1\n",
        ")\n",
        "\n",
        "# === Get correct feature names from preprocessor ===\n",
        "feature_names = best_model.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# === Create DataFrame of importances ===\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': perm_result.importances_mean\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# === Display top 20 features ===\n",
        "print(\"\\n Top 20 Most Important Features (SVM - Permutation Importance):\\n\")\n",
        "print(importance_df.head(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qW6rlw3y4Tn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a 2x2 grid of subplots\n",
        "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "# Plot 1\n",
        "axs[0, 0].plot(x1, y1)\n",
        "axs[0, 0].set_title('Plot 1')\n",
        "\n",
        "# Plot 2\n",
        "axs[0, 1].plot(x2, y2)\n",
        "axs[0, 1].set_title('Plot 2')\n",
        "\n",
        "# Plot 3\n",
        "axs[1, 0].plot(x3, y3)\n",
        "axs[1, 0].set_title('Plot 3')\n",
        "\n",
        "# Plot 4\n",
        "axs[1, 1].plot(x4, y4)\n",
        "axs[1, 1].set_title('Plot 4')\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "42eOoPG3lCMl",
        "outputId": "dfe0577e-b3e8-4a40-f5f9-96fd0a863d06"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# === Select top 20 features ===\n",
        "top_features = importance_df.head(20)\n",
        "\n",
        "# === Set plot style ===\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# === Create horizontal bar plot ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(\n",
        "    x=\"Importance\", y=\"Feature\",\n",
        "    data=top_features,\n",
        "    palette=\"Blues\"\n",
        ")\n",
        "\n",
        "# === Add plot labels and title ===\n",
        "plt.title(\"Feature Importance - SVM\", fontsize=14)\n",
        "plt.xlabel(\"Mean Importance\", fontsize=12)\n",
        "plt.ylabel(\"Feature\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "\n",
        "# === Show plot ===\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihh7y3jP1cjj"
      },
      "source": [
        "**Model Comparision on general feature set (5-fold)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VYIz9k81uY2"
      },
      "outputs": [],
      "source": [
        "###### most consistant features among all models that consistently appeared with notable importance in at least three out of four models\n",
        "\n",
        "general_features = [\n",
        "    \"switched_block3\",\n",
        "    \"switched_1_to_2\",\n",
        "    \"switched_2_to_3\",\n",
        "    \"pred_cluster_block3_1\",\n",
        "    \"pred_cluster_block3_2\",\n",
        "    \"pred_cluster_block1_2\",\n",
        "    \"pred_cluster_block1_1\",\n",
        "    \"pred_cluster_block2_1\",\n",
        "    \"kproto_cluster_1\",\n",
        "    \"kproto_cluster_2\",\n",
        "    \"age_income_interaction\",\n",
        "    \"age\",\n",
        "    \"income\",\n",
        "    \"education_low\",\n",
        "    \"gender_2\",\n",
        "    \"gender_3\"\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aXThG4ct7J0w",
        "outputId": "9835f939-b975-44a2-80ef-2ffacf8ae865"
      },
      "outputs": [],
      "source": [
        "######### Logistic Regression ##############\n",
        "\n",
        "# === 1. Load and preprocess dataset ===\n",
        "df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# Map education\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "df[\"education\"] = df[\"education\"].replace(education_map)\n",
        "df[\"education_low\"] = (df[\"education\"] == \"low\").astype(int)\n",
        "\n",
        "# Create interaction term\n",
        "df[\"age_income_interaction\"] = df[\"age\"] * df[\"income\"]\n",
        "\n",
        "# One-hot encode gender\n",
        "df[\"gender_2\"] = (df[\"gender\"] == 2).astype(int)\n",
        "df[\"gender_3\"] = (df[\"gender\"] == 3).astype(int)\n",
        "\n",
        "# One-hot encode cluster features\n",
        "cluster_cols = [\"kproto_cluster\", \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "df = pd.get_dummies(df, columns=cluster_cols, prefix=cluster_cols, drop_first=False)\n",
        "\n",
        "# Convert boolean columns to int\n",
        "bool_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "\n",
        "# Encode target\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(df[\"cond\"])\n",
        "\n",
        "# Drop unused columns\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\", \"cond\"\n",
        "]\n",
        "df = df.drop(columns=drop_cols)\n",
        "\n",
        "# === 2. Select general feature set ===\n",
        "general_features = [\n",
        "    \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\",\n",
        "    \"pred_cluster_block3_1\", \"pred_cluster_block3_2\",\n",
        "    \"pred_cluster_block1_2\", \"pred_cluster_block1_1\",\n",
        "    \"pred_cluster_block2_1\", \"kproto_cluster_1\", \"kproto_cluster_2\",\n",
        "    \"age_income_interaction\", \"age\", \"income\",\n",
        "    \"education_low\", \"gender_2\", \"gender_3\"\n",
        "]\n",
        "\n",
        "X_general = df[general_features]\n",
        "\n",
        "# === 3. Scale numeric features ===\n",
        "scaler = StandardScaler()\n",
        "scaled_cols = [\"income\", \"age\", \"age_income_interaction\"]\n",
        "X_general[scaled_cols] = scaler.fit_transform(X_general[scaled_cols])\n",
        "\n",
        "# === 4. Train/test split ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_general, y_encoded, test_size=0.2, stratify=df[\"gender\"], random_state=42\n",
        ")\n",
        "\n",
        "# === 5. Fit logistic regression ===\n",
        "log_reg = LogisticRegression(max_iter=1000, multi_class=\"multinomial\", solver=\"lbfgs\")\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# === 6. Evaluation ===\n",
        "print(\"\\n=== Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression (General Features)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# === 7. Cross-validation ===\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_acc = cross_val_score(log_reg, X_train, y_train, cv=skf, scoring=\"accuracy\")\n",
        "cv_f1 = cross_val_score(log_reg, X_train, y_train, cv=skf, scoring=\"f1_macro\")\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
        "print(f\"Test Macro F1: {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
        "print(f\"CV Accuracy (mean ± std): {cv_acc.mean():.3f} ± {cv_acc.std():.3f}\")\n",
        "print(f\"CV Macro F1 (mean ± std): {cv_f1.mean():.3f} ± {cv_f1.std():.3f}\")\n",
        "\n",
        "# === PCA Visualization with Class Labels ===\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Map encoded labels back to original class names\n",
        "class_names = label_encoder.classes_\n",
        "y_labels = [class_names[i] for i in y_encoded]\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# === Apply PCA to reduce general feature matrix to 2 components ===\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_general)\n",
        "\n",
        "# === Map encoded labels back to original class names ===\n",
        "class_names = label_encoder.classes_  # ['c', 'h', 'p', 'y']\n",
        "label_map = dict(enumerate(class_names))\n",
        "y_labels = [label_map[i] for i in y_encoded]\n",
        "\n",
        "# === Create a DataFrame for PCA scatter plot ===\n",
        "lr_pca_df = pd.DataFrame({\n",
        "    \"PC1\": X_pca[:, 0],\n",
        "    \"PC2\": X_pca[:, 1],\n",
        "    \"Class\": y_labels\n",
        "})\n",
        "\n",
        "# === Plot PCA scatter with class labels ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(\n",
        "    data=lr_pca_df,\n",
        "    x=\"PC1\", y=\"PC2\",\n",
        "    hue=\"Class\",\n",
        "    palette=\"Set2\",\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.title(\"PCA Visualization of General Feature Set (Logistic Regression)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title=\"Class\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "collapsed": true,
        "id": "lJ4JIRMznor-",
        "outputId": "40383a8c-52ca-4e4d-9d95-cf1f7f9f610a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Apply PCA to reduce general feature matrix to 2 components\n",
        "# pca = PCA(n_components=2)\n",
        "# X_pca = pca.fit_transform(X_general)\n",
        "\n",
        "# # Map encoded labels back to original class names\n",
        "# class_names = label_encoder.classes_  # ['c', 'h', 'p', 'y'] — adjust if needed\n",
        "# label_map = dict(enumerate(class_names))\n",
        "# y_labels = [label_map[i] for i in y_encoded]\n",
        "\n",
        "# # Create a DataFrame for PCA scatter plot\n",
        "# lr_pca_df = pd.DataFrame({\n",
        "#     \"PC1\": X_pca[:, 0],\n",
        "#     \"PC2\": X_pca[:, 1],\n",
        "#     \"Class\": y_labels\n",
        "# })\n",
        "\n",
        "# # Plot PCA scatter with class labels\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# sns.scatterplot(\n",
        "#     data=lr_pca_df,\n",
        "#     x=\"PC1\", y=\"PC2\",\n",
        "#     hue=\"Class\",\n",
        "#     palette=\"Set2\",\n",
        "#     alpha=0.7\n",
        "# )\n",
        "# plt.title(\"PCA Visualization of General Feature Set (Logistic Regression)\")\n",
        "# plt.xlabel(\"Principal Component 1\")\n",
        "# plt.ylabel(\"Principal Component 2\")\n",
        "# plt.legend(title=\"Class\")\n",
        "# plt.grid(True)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmTaKCsd9_1S",
        "outputId": "9e5041e4-3242-44ae-d589-c79bcfe8b1f4"
      },
      "outputs": [],
      "source": [
        "################# Random Forest ########################\n",
        "\n",
        "\n",
        "# === 1. Load dataset ===\n",
        "df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# === 2. Encode target ===\n",
        "y = df[\"cond\"]\n",
        "label_encoder = LabelEncoder()\n",
        "fr_y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# === 3. Feature engineering ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "df[\"education\"] = df[\"education\"].replace(education_map)\n",
        "df[\"education_low\"] = (df[\"education\"] == \"low\").astype(int)\n",
        "df[\"age_income_interaction\"] = df[\"age\"] * df[\"income\"]\n",
        "df[\"gender_2\"] = (df[\"gender\"] == 2).astype(int)\n",
        "df[\"gender_3\"] = (df[\"gender\"] == 3).astype(int)\n",
        "\n",
        "# One-hot encode cluster features\n",
        "cluster_cols = [\"kproto_cluster\", \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "df = pd.get_dummies(df, columns=cluster_cols, prefix=cluster_cols, drop_first=False)\n",
        "\n",
        "# Convert boolean columns to int\n",
        "bool_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\", \"cond\"\n",
        "]\n",
        "df = df.drop(columns=drop_cols)\n",
        "\n",
        "# === 4. Select general feature set ===\n",
        "general_features = [\n",
        "    \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\",\n",
        "    \"pred_cluster_block3_1\", \"pred_cluster_block3_2\",\n",
        "    \"pred_cluster_block1_2\", \"pred_cluster_block1_1\",\n",
        "    \"pred_cluster_block2_1\", \"kproto_cluster_1\", \"kproto_cluster_2\",\n",
        "    \"age_income_interaction\", \"age\", \"income\",\n",
        "    \"education_low\", \"gender_2\", \"gender_3\"\n",
        "]\n",
        "\n",
        "X = df[general_features]\n",
        "\n",
        "# === 5. Preprocessing and pipeline ===\n",
        "scaled_cols = [\"income\", \"age\", \"age_income_interaction\"]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), scaled_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# === 6. Hyperparameter tuning ===\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [5, 10, 20, None],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "cv_grid = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "rf_grid_search = GridSearchCV(model_pipeline, param_grid, cv=cv_grid, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
        "rf_grid_search.fit(X, y_encoded)\n",
        "\n",
        "rf_best_model = grid_search.best_estimator_\n",
        "rf_val_score = grid_search.best_score_\n",
        "\n",
        "print(\"\\nBest Random Forest Parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best CV Macro F1 Score (Train CV): {rf_val_score:.3f}\")\n",
        "\n",
        "# === 7. 5-Fold Evaluation ===\n",
        "rf_acc_scores = cross_val_score(best_model, X, fr_y_encoded, cv=cv_grid, scoring='accuracy')\n",
        "rf_f1_scores = cross_val_score(best_model, X, fr_y_encoded, cv=cv_grid, scoring='f1_macro')\n",
        "\n",
        "print(\"\\n=== Random Forest Performance (5-Fold CV) ===\")\n",
        "for i, (acc, f1) in enumerate(zip(rf_acc_scores, rf_f1_scores), 1):\n",
        "    print(f\"Fold {i}: Accuracy = {acc:.4f}, Macro F1 = {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nCV Accuracy (mean ± std): {rf_acc_scores.mean():.3f} ± {rf_acc_scores.std():.3f}\")\n",
        "print(f\"CV Macro F1 (mean ± std): {rf_f1_scores.mean():.3f} ± {rf_f1_scores.std():.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "wNsY0_Tbohjh",
        "outputId": "addb01ea-d7dc-4d0d-c36b-068557cd8922"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Apply PCA to reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "rf_X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Map encoded labels back to class names\n",
        "class_names = label_encoder.inverse_transform([0, 1, 2, 3])  # Adjust if needed\n",
        "label_map = dict(zip(range(len(class_names)), class_names))\n",
        "fr_y_labels = [label_map[i] for i in fr_y_encoded]\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "rf_pca_df = pd.DataFrame({\n",
        "    \"PC1\": rf_X_pca[:, 0],\n",
        "    \"PC2\": rf_X_pca[:, 1],\n",
        "    \"Class\": fr_y_labels\n",
        "})\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(data=rf_pca_df, x=\"PC1\", y=\"PC2\", hue=\"Class\", palette=\"Set2\", alpha=0.7)\n",
        "plt.title(\"PCA Visualization of General Feature Set (Random Forest)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title=\"Class\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9BJWCJcekUc",
        "outputId": "0bf7ccd7-affd-4b16-bd22-a8192578b48b"
      },
      "outputs": [],
      "source": [
        "# === 10. Report results ===\n",
        "print(\"\\n=== Random Forest Performance Summary ===\")\n",
        "print(f\"CV Accuracy (Mean ± Std): {rf_acc_scores.mean():.3f} ± {rf_acc_scores.std():.3f}\")\n",
        "print(f\"CV Macro F1 (Mean ± Std): {rf_f1_scores.mean():.3f} ± {rf_f1_scores.std():.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Macro F1: {test_macro_f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMze5m3b-G-K",
        "outputId": "f92e19e6-bf39-4771-f360-fe8d0f60d089"
      },
      "outputs": [],
      "source": [
        "################# LightGBM #######################\n",
        "\n",
        "\n",
        "# === 1. Load dataset ===\n",
        "df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# === 2. Encode target ===\n",
        "y = df[\"cond\"]\n",
        "label_encoder = LabelEncoder()\n",
        "light_y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# === 3. Feature engineering ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "df[\"education\"] = df[\"education\"].replace(education_map)\n",
        "df[\"education_low\"] = (df[\"education\"] == \"low\").astype(int)\n",
        "df[\"age_income_interaction\"] = df[\"age\"] * df[\"income\"]\n",
        "df[\"gender_2\"] = (df[\"gender\"] == 2).astype(int)\n",
        "df[\"gender_3\"] = (df[\"gender\"] == 3).astype(int)\n",
        "\n",
        "# One-hot encode cluster features\n",
        "cluster_cols = [\"kproto_cluster\", \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "df = pd.get_dummies(df, columns=cluster_cols, prefix=cluster_cols, drop_first=False)\n",
        "\n",
        "# Convert boolean columns to int\n",
        "bool_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\", \"cond\"\n",
        "]\n",
        "df = df.drop(columns=drop_cols)\n",
        "\n",
        "# === 4. Select general feature set ===\n",
        "general_features = [\n",
        "    \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\",\n",
        "    \"pred_cluster_block3_1\", \"pred_cluster_block3_2\",\n",
        "    \"pred_cluster_block1_2\", \"pred_cluster_block1_1\",\n",
        "    \"pred_cluster_block2_1\", \"kproto_cluster_1\", \"kproto_cluster_2\",\n",
        "    \"age_income_interaction\", \"age\", \"income\",\n",
        "    \"education_low\", \"gender_2\", \"gender_3\"\n",
        "]\n",
        "\n",
        "X = df[general_features]\n",
        "\n",
        "# === 5. Define LightGBM model ===\n",
        "light_model = LGBMClassifier(random_state=42, class_weight='balanced')\n",
        "\n",
        "# === 6. 5-Fold Cross-Validation ===\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "light_acc_scores = cross_val_score(light_model, X, light_y_encoded, cv=cv, scoring='accuracy')\n",
        "light_f1_scores = cross_val_score(light_model, X, light_y_encoded, cv=cv, scoring='f1_macro')\n",
        "\n",
        "# === 7. Report results ===\n",
        "print(\"\\n=== LightGBM Performance (5-Fold CV) ===\")\n",
        "for i, (acc, f1) in enumerate(zip(light_acc_scores, light_f1_scores), 1):\n",
        "    print(f\"Fold {i}: Accuracy = {acc:.4f}, Macro F1 = {f1:.4f}\")\n",
        "\n",
        "print(f\"\\nCV Accuracy (mean ± std): {light_acc_scores.mean():.3f} ± {light_acc_scores.std():.3f}\")\n",
        "print(f\"CV Macro F1 (mean ± std): {light_f1_scores.mean():.3f} ± {light_f1_scores.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "eEx5W5MgpGLA",
        "outputId": "ffd2a35f-488b-4c5f-d285-8bf026abbc4b"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === 1. Apply PCA ===\n",
        "light_pca = PCA(n_components=2)\n",
        "light_X_pca = pca.fit_transform(X)\n",
        "\n",
        "# === 2. Map encoded labels to class names ===\n",
        "class_names = label_encoder.inverse_transform([0, 1, 2, 3])  # Adjust if needed\n",
        "label_map = dict(zip(range(len(class_names)), class_names))\n",
        "light_y_labels = [label_map[i] for i in light_y_encoded]\n",
        "\n",
        "# === 3. Create DataFrame for plotting ===\n",
        "light_pca_df = pd.DataFrame({\n",
        "    \"PC1\": light_X_pca[:, 0],\n",
        "    \"PC2\": light_X_pca[:, 1],\n",
        "    \"Class\": light_y_labels\n",
        "})\n",
        "\n",
        "# === 4. Plot PCA scatter ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(data=light_pca_df, x=\"PC1\", y=\"PC2\",  hue=\"Class\", palette=\"Set2\", alpha=0.7)\n",
        "plt.title(\"PCA Visualization of General Feature Set (LightGBM)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title=\"Class\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ZQvCepc66c",
        "outputId": "5d384c5e-edf1-4b0b-9b58-c8923a482f59"
      },
      "outputs": [],
      "source": [
        "# === 8. Fit and evaluate on test set ===\n",
        "light_model.fit(X_train, y_train)\n",
        "light_y_test_pred = model.predict(X_test)\n",
        "light_test_accuracy = accuracy_score(y_test, light_y_test_pred)\n",
        "light_test_macro_f1 = f1_score(y_test, light_y_test_pred, average='macro')\n",
        "\n",
        "# === 9. Report results ===\n",
        "print(\"\\n=== LightGBM Performance Summary ===\")\n",
        "print(f\"CV Accuracy (Mean ± Std): {light_acc_scores.mean():.3f} ± {light_acc_scores.std():.3f}\")\n",
        "print(f\"CV Macro F1 (Mean ± Std): {light_f1_scores.mean():.3f} ± {light_f1_scores.std():.3f}\")\n",
        "print(f\"Test Accuracy: {light_test_accuracy:.3f}\")\n",
        "print(f\"Test Macro F1: {light_test_macro_f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlLyMbCX-LFi",
        "outputId": "cad36b51-e4be-4d56-b5a1-0ea0aac1fd85"
      },
      "outputs": [],
      "source": [
        "################ SVM #####################\n",
        "\n",
        "\n",
        "# === 1. Load dataset ===\n",
        "try:\n",
        "    df = pd.read_csv(\"modeling_dataset_with_demo_and_age.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'modeling_dataset_with_demo_and_age.csv' not found.\")\n",
        "    exit()\n",
        "\n",
        "df = df.dropna(subset=[\"cond\"])\n",
        "\n",
        "# === 2. Encode target ===\n",
        "y = df[\"cond\"]\n",
        "label_encoder = LabelEncoder()\n",
        "svm_y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# === 3. Map education and create interaction features ===\n",
        "education_map = {1: \"low\", 2: \"low\", 3: \"low\", 4: \"high\", 5: \"high\"}\n",
        "df[\"education\"] = df[\"education\"].replace(education_map)\n",
        "df[\"education_low\"] = (df[\"education\"] == \"low\").astype(int)\n",
        "df[\"age_income_interaction\"] = df[\"age\"] * df[\"income\"]\n",
        "\n",
        "# === 4. One-hot encode gender ===\n",
        "df[\"gender_2\"] = (df[\"gender\"] == 2).astype(int)\n",
        "df[\"gender_3\"] = (df[\"gender\"] == 3).astype(int)\n",
        "\n",
        "# === 5. One-hot encode cluster features ===\n",
        "cluster_cols = [\"kproto_cluster\", \"pred_cluster_block1\", \"pred_cluster_block2\", \"pred_cluster_block3\"]\n",
        "df = pd.get_dummies(df, columns=cluster_cols, prefix=cluster_cols, drop_first=False)\n",
        "\n",
        "# === 6. Convert boolean columns to int ===\n",
        "bool_cols = [\"switched_block1\", \"switched_block2\", \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\"]\n",
        "df[bool_cols] = df[bool_cols].astype(int)\n",
        "\n",
        "# === 7. Drop unnecessary columns ===\n",
        "drop_cols = [\n",
        "    \"subj_num\", \"transition_block1\", \"transition_block2\", \"transition_block3\",\n",
        "    \"transition_1_to_2\", \"transition_2_to_3\", \"total_switches\", \"sequential_switches\", \"cond\"\n",
        "]\n",
        "df = df.drop(columns=drop_cols)\n",
        "\n",
        "# === 8. Define general feature set ===\n",
        "general_features = [\n",
        "    \"switched_block3\", \"switched_1_to_2\", \"switched_2_to_3\",\n",
        "    \"pred_cluster_block3_1\", \"pred_cluster_block3_2\",\n",
        "    \"pred_cluster_block1_2\", \"pred_cluster_block1_1\",\n",
        "    \"pred_cluster_block2_1\", \"kproto_cluster_1\", \"kproto_cluster_2\",\n",
        "    \"age_income_interaction\", \"age\", \"income\",\n",
        "    \"education_low\", \"gender_2\", \"gender_3\"\n",
        "]\n",
        "\n",
        "X = df[general_features]\n",
        "\n",
        "# === 9. Train/test split ===\n",
        "svm_X_train, svm_X_test, svm_y_train, y_test = train_test_split(\n",
        "    X, svm_y_encoded, test_size=0.2, stratify=df[\"gender\"], random_state=42\n",
        ")\n",
        "\n",
        "# === 10. Preprocessing and SVM pipeline ===\n",
        "scaled_cols = [\"income\", \"age\", \"age_income_interaction\"]\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), scaled_cols)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC(random_state=42))\n",
        "])\n",
        "\n",
        "# === 11. Grid search for hyperparameter tuning ===\n",
        "print(\"\\n=== Tuning SVM with GridSearchCV ===\")\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'classifier__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'classifier__kernel': ['rbf']\n",
        "}\n",
        "\n",
        "cv_grid = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv_grid,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "val_score = grid_search.best_score_\n",
        "\n",
        "print(f\"\\nBest SVM Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV Macro F1 Score (Train CV): {val_score:.3f}\")\n",
        "\n",
        "# === 12. Cross-validation std for best config ===\n",
        "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "mask = (\n",
        "    (cv_results['param_classifier__C'] == best_params['classifier__C']) &\n",
        "    (cv_results['param_classifier__gamma'] == best_params['classifier__gamma']) &\n",
        "    (cv_results['param_classifier__kernel'] == best_params['classifier__kernel'])\n",
        ")\n",
        "\n",
        "best_cv_scores = cv_results.loc[mask, 'mean_test_score']\n",
        "best_cv_std = cv_results.loc[mask, 'std_test_score']\n",
        "\n",
        "print(f\"CV Macro F1 (mean ± std): {best_cv_scores.values[0]:.3f} ± {best_cv_std.values[0]:.3f}\")\n",
        "\n",
        "# === 13. Evaluate on test set ===\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"\\nTest Accuracy (Held-Out): {test_accuracy:.3f}\")\n",
        "print(\"\\n=== Classification Report (Test Set) ===\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "uG_IuZWmpg6E",
        "outputId": "04439be2-1e7e-4a19-a772-4e0b9da05f97"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# === 1. Apply PCA ===\n",
        "pca = PCA(n_components=2)\n",
        "svm_X_pca = pca.fit_transform(X)\n",
        "\n",
        "# === 2. Map encoded labels to class names ===\n",
        "class_names = label_encoder.inverse_transform([0, 1, 2, 3])  # Adjust if needed\n",
        "label_map = dict(zip(range(len(class_names)), class_names))\n",
        "y_labels = [label_map[i] for i in y_encoded]\n",
        "\n",
        "# === 3. Create DataFrame for plotting ===\n",
        "svm_pca_df = pd.DataFrame({\n",
        "    \"PC1\": X_pca[:, 0],\n",
        "    \"PC2\": X_pca[:, 1],\n",
        "    \"Class\": y_labels\n",
        "})\n",
        "\n",
        "# === 4. Plot PCA scatter ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(data=svm_pca_df, x=\"PC1\", y=\"PC2\", hue=\"Class\", palette=\"Set2\", alpha=0.7)\n",
        "plt.title(\"PCA Visualization of General Feature Set (SVM)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title=\"Class\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y8_hGFQ20vK8",
        "outputId": "9d8de5b8-a4b1-4dc0-b2a9-eaadf0e97d45"
      },
      "outputs": [],
      "source": [
        "# Skip re-creating PCA DataFrames — use them directly\n",
        "dfs = [lr_pca_df, rf_pca_df, light_pca_df, svm_pca_df]\n",
        "titles = [\"Logistic Regression\", \"Random Forest\", \"LightGBM\", \"SVM\"]\n",
        "\n",
        "# Plot all in a 2x2 grid\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "for ax, df, title in zip(axes.flatten(), dfs, titles):\n",
        "    sns.scatterplot(data=df, x=\"PC1\", y=\"PC2\", hue=\"Class\", palette=\"Set1\", alpha=0.7, ax=ax)\n",
        "    ax.set_title(f\"PCA Visualization ({title})\")\n",
        "    ax.set_xlabel(\"Principal Component 1\")\n",
        "    ax.set_ylabel(\"Principal Component 2\")\n",
        "    ax.legend(title=\"Class\", loc='best')\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UHkZDyBbzyM",
        "outputId": "57f8642d-9325-4625-c2ab-add0ae100977"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === CV Accuracy (Mean ± Std) ===\n",
        "cv_accuracy_mean = cv_results.loc[mask, 'mean_test_score'].values[0]\n",
        "cv_accuracy_std = cv_results.loc[mask, 'std_test_score'].values[0]\n",
        "\n",
        "# === CV Macro F1 (Mean ± Std) ===\n",
        "cv_macro_f1_mean = cv_results.loc[mask, 'mean_test_score'].values[0]\n",
        "cv_macro_f1_std = cv_results.loc[mask, 'std_test_score'].values[0]\n",
        "\n",
        "# === Test Accuracy ===\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "# === Test Macro F1 ===\n",
        "test_macro_f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "# === Print all metrics ===\n",
        "print(\"\\n=== SVM Performance Summary ===\")\n",
        "print(f\"CV Accuracy (Mean ± Std): {cv_accuracy_mean:.3f} ± {cv_accuracy_std:.3f}\")\n",
        "print(f\"CV Macro F1 (Mean ± Std): {cv_macro_f1_mean:.3f} ± {cv_macro_f1_std:.3f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Macro F1: {test_macro_f1:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry8m0cBXTgcI"
      },
      "source": [
        "**Algo-specific Model Comparision, 3-folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43igJ_LoToxK"
      },
      "outputs": [],
      "source": [
        "\n",
        "models = {\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "param_grids = {\n",
        "    \"SVM\": {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__gamma': [0.01, 0.1],\n",
        "        'classifier__kernel': ['rbf']\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [5, 10]\n",
        "    },\n",
        "    \"LogisticRegression\": {\n",
        "        'classifier__C': [0.1, 1, 10]\n",
        "    },\n",
        "    \"LightGBM\": {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [3, 5],\n",
        "        'classifier__learning_rate': [0.01, 0.1]\n",
        "    }\n",
        "}\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    grid = param_grids[name]\n",
        "    grid_search = GridSearchCV(pipeline, grid, cv=3, scoring='f1_macro', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Best Params\": grid_search.best_params_,\n",
        "        \"CV Macro F1\": grid_search.best_score_,\n",
        "        \"Test Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Test Macro F1\": f1_score(y_test, y_pred, average='macro')\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(results_df.sort_values(by=\"Test Macro F1\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vll3Tma-UjqJ"
      },
      "source": [
        "**Algo-specific Model Comparision, 5-folds**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQi72X-4UxHW"
      },
      "outputs": [],
      "source": [
        "\n",
        "models = {\n",
        "    \"SVM\": SVC(random_state=42),\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
        "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "\n",
        "param_grids = {\n",
        "    \"SVM\": {\n",
        "        'classifier__C': [0.1, 1, 10],\n",
        "        'classifier__gamma': [0.01, 0.1],\n",
        "        'classifier__kernel': ['rbf']\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [5, 10]\n",
        "    },\n",
        "    \"LogisticRegression\": {\n",
        "        'classifier__C': [0.1, 1, 10]\n",
        "    },\n",
        "    \"LightGBM\": {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__max_depth': [3, 5],\n",
        "        'classifier__learning_rate': [0.01, 0.1]\n",
        "    }\n",
        "}\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', model)\n",
        "    ])\n",
        "\n",
        "    grid = param_grids[name]\n",
        "    grid_search = GridSearchCV(pipeline, grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Best Params\": grid_search.best_params_,\n",
        "        \"CV Macro F1\": grid_search.best_score_,\n",
        "        \"Test Accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"Test Macro F1\": f1_score(y_test, y_pred, average='macro')\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(results_df.sort_values(by=\"Test Macro F1\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6UHv3yOduYK"
      },
      "source": [
        "**Testing model performance (best model, LightGBM) for gender**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARy7ZVPrd3xC"
      },
      "outputs": [],
      "source": [
        "gender_test = X.loc[X_test.index, \"gender\"]\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"\\n=== Gender-Based Performance ===\")\n",
        "for gender_value in sorted(gender_test.unique()):\n",
        "    mask = gender_test == gender_value\n",
        "    y_true_gender = y_test[mask]\n",
        "    y_pred_gender = y_test_pred[mask]\n",
        "\n",
        "    print(f\"\\n--- Gender: {gender_value} ---\")\n",
        "    print(f\"Support: {len(y_true_gender)}\")\n",
        "    print(classification_report(y_true_gender, y_pred_gender, target_names=label_encoder.classes_))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
